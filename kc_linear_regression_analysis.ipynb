{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Student name: Spencer Hadel\n",
    "* Student pace: self paced\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: \n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Problem:\n",
    "\n",
    "TENTATIVE:\n",
    "\n",
    "A new real estate company that would like to use the data on past house sales to inform their decisions on how to properly price homes for prospective sellers and buyers in the area. They would like to look at the effect of square footage, numbers of bedrooms and bathrooms, waterfront views, recent renovations, and possibly location to inform their decisions on what the appropriate market value of a home will be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Prepared Data from kc_data_cleaning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already cleaned the King County Housing Data in the kc_data_cleaning notebook:\n",
    "\n",
    "[Data Cleaning Notebook!](./kc_data_cleaning)\n",
    "\n",
    "We only need the cleaned_df dataframe from it, imported below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/cleaned.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 21534 entries, 0 to 21596\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   price           21534 non-null  float64\n",
      " 1   bedrooms        21534 non-null  int64  \n",
      " 2   bathrooms       21534 non-null  float64\n",
      " 3   sqft_living     21534 non-null  int64  \n",
      " 4   sqft_lot        21534 non-null  int64  \n",
      " 5   floors          21534 non-null  float64\n",
      " 6   waterfront      21534 non-null  object \n",
      " 7   view            21534 non-null  object \n",
      " 8   condition       21534 non-null  object \n",
      " 9   grade           21534 non-null  object \n",
      " 10  yr_built        21534 non-null  int64  \n",
      " 11  renovated_2000  21534 non-null  int64  \n",
      "dtypes: float64(3), int64(5), object(4)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Categorical Data\n",
    "\n",
    "We need to find out which of the features contain categorical data. It is important to identify this because we want our model to be trained on normalized numerical values, not human concepts like condition scores. The most obvious categorical variables above are the ones where the dtype is an object. However, there are ints and floats that also pertain to categorical data, such as the number of bedrooms. \n",
    "\n",
    "We don't want our model predicting that a house's value will go up with the addition of 0.33 bedrooms, so we need to break down features like this into dummy variables. While we can look at the features and use common sense to identify categoricals, visualizing the data can also help with this, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shade\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\tools.py:331: MatplotlibDeprecationWarning: \n",
      "The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead.\n",
      "  if ax.is_first_col():\n"
     ]
    }
   ],
   "source": [
    "df.hist(figsize=[15,15], bins='auto');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any feature that shows seperate individual columns of data clearly indicates that it is categorical. We create a new subset of the data composed of these features and the features that have a dtype of object.\n",
    "\n",
    "We convert the new df to all strings in order to aid in the dummy creation process in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df = df[['bedrooms', 'bathrooms', 'floors', 'renovated_2000', 'grade',\n",
    "        'condition', 'view', 'waterfront']].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding Categoricals with Dummy Variables\n",
    "\n",
    "We need to use one-hot encoding to split these features into data useable by our model. We create dummy variables for each potential variable of the categorical data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dummies_df = pd.get_dummies(categorical_df, drop_first=True)\n",
    "dummies_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that our categorical features has been successfully split into unique dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Data\n",
    "\n",
    "The remaining features are continuous numerical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "continuous_df = df[['price','sqft_living','sqft_lot', 'yr_built']]\n",
    "\n",
    "continuous_df.hist(figsize=[15,3], bins='auto', layout=(1,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming and Standardizing Data\n",
    "\n",
    "This data is extremely skewed, so we will use the log transform to make the data more normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "continuous_log = np.log(continuous_df)\n",
    "continuous_log.hist(figsize=[15,3], bins='auto', layout=(1,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been transformed, we standardize it in order to properly scale the continuous features in relation to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(feature):\n",
    "    return (feature - feature.mean()) / feature.std()\n",
    "#normalize data\n",
    "\n",
    "normalized_cont = continuous_log.apply(normalize)\n",
    "normalized_cont.hist(figsize=[15,3], bins='auto', layout=(1,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test Data\n",
    "\n",
    "Now that we have a complete preprocessed dataset, we need to split the data into train and test datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
